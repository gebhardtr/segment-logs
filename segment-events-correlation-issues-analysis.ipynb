{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event correlation issues analysis (Analytics/Schema2 Telemetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronize all folders from S3 bucket into the data/ dir.\n",
    "\n",
    "Example:\n",
    "\n",
    "- data/\n",
    "  - 1709164800000/\n",
    "    - gzip files\n",
    "  - 1709251200000/\n",
    "    - gzip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all folders inside data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataframes_root_path = \"data\"\n",
    "\n",
    "files_dir = [\n",
    "    f for f in os.listdir(dataframes_root_path) if os.path.isdir(os.path.join(dataframes_root_path, f))\n",
    "]\n",
    "print(files_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all.jsonl files (per folder & global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cmd = f\"rm -f ./data/all.jsonl\"\n",
    "\n",
    "for dir in files_dir:\n",
    "    target_all_file = f\"./data/{dir}/all.jsonl\"\n",
    "    cmd = f\"rm -f {target_all_file}\"\n",
    "    os.system(cmd)\n",
    "\n",
    "for dir in files_dir:\n",
    "    target_dir = f\"./data/{dir}\"\n",
    "    print(f\"Processing dir: {target_dir}...\")\n",
    "    cmd = f\"find {target_dir} -type f | parallel --bar 'zcat {{}} | jq -c \\\".\\\"' > {target_dir}/all.jsonl\"\n",
    "    os.system(cmd)\n",
    "    cmd = f\"cat {target_dir}/all.jsonl >> ./data/all.jsonl\"\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import & Initialize PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "sc.getConf().set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "sqlContext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log every folder's timestamp, and timestamps from events contained (distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def process_dir(dir):\n",
    "    target_all_file = f\"./data/{dir}/all.jsonl\"\n",
    "    folder_date = datetime.fromtimestamp(int(dir) / 1000.0)\n",
    "    dataframe_all = sqlContext.read.json(target_all_file)\n",
    "    print(\"==============================================================\")\n",
    "    print(f\"=====> {folder_date}\")\n",
    "    print(f\"       COUNT = {dataframe_all.count()}\")\n",
    "    dataframe_all.select(date_format(col(\"timestamp\"), \"MM-dd-yyyy\").alias(\"date_format\")).distinct().show()\n",
    "    print(\"==============================================================\")\n",
    "\n",
    "for dir in files_dir:\n",
    "    process_dir(dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all schema2 events from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array_size\n",
    "\n",
    "target_all_file = f\"./data/all.jsonl\"\n",
    "dataframe_all = sqlContext.read.json(target_all_file)\n",
    "\n",
    "completions = dataframe_all.filter(\"event == 'Recommendation Generated'\").alias(\"completions\")\n",
    "action_feedbacks = dataframe_all.filter(\"event == 'Recommendation Action'\").alias(\"action_feedbacks\")\n",
    "product_feedbacks = dataframe_all.filter(\"event == 'Product Feedback'\").alias(\"product_feedbacks\")\n",
    "\n",
    "total_events = dataframe_all.count()\n",
    "print(f\"Total Events: #{total_events}\")\n",
    "total_completions = completions.count()\n",
    "print(f\"Total Completions: #{total_completions}\")\n",
    "total_action_feedbacks = action_feedbacks.count()\n",
    "print(f\"Total Action Feedbacks: #{total_action_feedbacks}\")\n",
    "total_product_feedbacks = product_feedbacks.count()\n",
    "print(f\"Total Product Feedbacks: #{total_product_feedbacks}\")\n",
    "#computed_events_count = total_completions + total_action_feedbacks + total_product_feedbacks\n",
    "#print(f\"Computed Events: #{computed_events_count}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out missing completion - feedback correlated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array_size\n",
    "\n",
    "completions_correlated_feedbacks = completions.join(action_feedbacks,\n",
    "                                     on= col(\"completions.properties.suggestion_id\") == col(\"action_feedbacks.properties.suggestion_id\"),\n",
    "                                     how=\"inner\")\n",
    "total_completions_correlated_feedbacks = completions_correlated_feedbacks.count()\n",
    "print(f\"Total correlated Completions-Feedback: #{total_completions_correlated_feedbacks}\")\n",
    "#completions_correlated_feedbacks.show()\n",
    "#completions_correlated_feedbacks.select([\"completions.properties.suggestion_id\"]).show(truncate=False)\n",
    "\n",
    "completions_missing_feedbacks = completions.join(action_feedbacks,\n",
    "                                     on= col(\"completions.properties.suggestion_id\") == col(\"action_feedbacks.properties.suggestion_id\"),\n",
    "                                     how=\"left_anti\")\n",
    "total_completions_missing_feedbacks = completions_missing_feedbacks.count()\n",
    "print(f\"Total Completions Missing Feedback: #{total_completions_missing_feedbacks}\")\n",
    "#completions_missing_feedbacks.show()\n",
    "#completions_missing_feedbacks.select([\"completions.properties.suggestion_id\"]).show(truncate=False)\n",
    "\n",
    "feedbacks_missing_completions = action_feedbacks.join(completions,\n",
    "                                     on= col(\"completions.properties.suggestion_id\") == col(\"action_feedbacks.properties.suggestion_id\"),\n",
    "                                     how=\"left_anti\")\n",
    "total_feedbacks_missing_completions = feedbacks_missing_completions.count()\n",
    "print(f\"Total Feedback Missing Completions: #{total_feedbacks_missing_completions}\")\n",
    "# feedbacks_missing_completions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out which RH org-ids are sending completions, but no correlated feedback events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at: userId, timestamp, originalTimestamp, receivedAt, sentAt, properties.suggestion_id\n",
    "completions_missing_feedbacks_tests = completions.join(action_feedbacks,\n",
    "                                     on= col(\"completions.properties.suggestion_id\") == col(\"action_feedbacks.properties.suggestion_id\"),\n",
    "                                     how=\"left_anti\")\n",
    "total_completions_missing_feedbacks_tests = completions_missing_feedbacks_tests.count()\n",
    "print(f\"Total Completions Missing Feedback: #{total_completions_missing_feedbacks_tests}\")\n",
    "\n",
    "completions_missing_feedbacks_tests.groupBy([\"completions.properties.rh_user_org_id\"]).count().orderBy(\"count\", ascending=False).show(n=20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "dir = \"1709164800000\"\n",
    "print(\"************* FOLDER :: 1709164800000 **************\")\n",
    "target_all_file = f\"./data/{dir}/all.jsonl\"\n",
    "folder_date = datetime.fromtimestamp(int(dir) / 1000.0)\n",
    "print(f\"Folder date: {folder_date}\")\n",
    "\n",
    "dataframe_all = sqlContext.read.json(target_all_file)\n",
    "completions = dataframe_all.filter(\"event == 'Recommendation Generated'\").alias(\"completions\")\n",
    "action_feedbacks = dataframe_all.filter(\"event == 'Recommendation Action'\").alias(\"action_feedbacks\")\n",
    "\n",
    "total_completions = completions.count()\n",
    "print(f\"Total Completions: #{total_completions}\")\n",
    "total_action_feedbacks = action_feedbacks.count()\n",
    "print(f\"Total Action Feedbacks: #{total_action_feedbacks}\")\n",
    "\n",
    "# https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\n",
    "completions_missing_feedbacks = completions.join(action_feedbacks,\n",
    "                                     on= col(\"completions.properties.suggestion_id\") == col(\"action_feedbacks.properties.suggestion_id\"),\n",
    "                                     how=\"left_anti\")\n",
    "total_completions_missing_feedbacks = completions_missing_feedbacks.count()\n",
    "print(f\"Total Completions Missing Feedback: #{total_completions_missing_feedbacks}\")\n",
    "#completions_missing_feedbacks.show()\n",
    "#completions_missing_feedbacks.select([\"completions.properties.suggestion_id\"]).show(truncate=False)\n",
    "\n",
    "\n",
    "completions_missing_feedbacks.groupBy([\"completions.properties.rh_user_org_id\"]).count().orderBy(\"count\", ascending=False).show(n=20, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
